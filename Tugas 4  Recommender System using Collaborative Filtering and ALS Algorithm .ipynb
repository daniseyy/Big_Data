{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas 4 Recommender System using Collaborative Filtering (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets \n",
    "I will be using some publicly available song data from audioscrobbler, which can be found here (https://old.datahub.io/dataset/audioscrobbler). However, i modified the original data files so that the code will run in a reasonable time on a single machine. The reduced data files have been suffixed with _small.txt and contains only the information relevant to the top 50 most prolific users (highest artist play counts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Package Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "from operator import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data from the Source in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc =SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data \n",
    "I'm going to load the 3 datasets into RDDs,."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(s, delimeters=\" \", to_int=None):\n",
    "    s = s.split(delimeters)\n",
    "    if to_int:\n",
    "        return tuple([int(s[i]) if i in to_int else s[i] for i in range(len(s))])\n",
    "    return tuple(s)\n",
    "artistData = sc.textFile(\"artist_data_small.txt\").map(lambda x: parser(x,'\\t',[0]))\n",
    "artistAlias = sc.textFile(\"artist_alias_small.txt\").map(lambda x: parser(x,'\\t', [0,1]))\n",
    "artistAliasMap = artistAlias.collectAsMap()\n",
    "userArtistData = sc.textFile(\"user_artist_data_small.txt\").map(lambda x: parser(x,' ',[0,1,2]))\n",
    "userArtistData = userArtistData.map(lambda x: (x[0], artistAliasMap.get(x[1], x[1]), x[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration \n",
    "Example : I will find the users' total play counts.  Find the three users with the highest number of total play counts (sum of all counters) and print the user ID, the total play count, and the mean play count (average number of times a user played an artist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1059637 has a total play count of 674412 and a mean play count of 1878.5849582172702.\n",
      "User 2064012 has a total play count of 548427 and a mean play count of 9455.637931034482.\n",
      "User 2069337 has a total play count of 393515 and a mean play count of 1519.3629343629343.\n"
     ]
    }
   ],
   "source": [
    "def summary(user_id):\n",
    "    play_list = userArtistData.map(lambda x: (x[0], (x[1], x[2]))).lookup(user_id)\n",
    "    total = sum(x[1] for x in play_list)\n",
    "    d = \"User %s has a total play count of %s and a mean play count of %s.\" % (user_id, total, total/len(play_list),)\n",
    "    print (d)\n",
    "summary(1059637)\n",
    "summary(2064012)\n",
    "summary(2069337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data for Testing \n",
    "Use the randomSplit function to divide the data (userArtistData) into:\n",
    "\n",
    "1. A training set, trainData, that will be used to train the model. This set should constitute 40% of the data.\n",
    "2. A validation set, validationData, used to perform parameter tuning. This set should constitute 40% of the data.\n",
    "3. A test set, testData, used for a final evaluation of the model. This set should constitute 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData, validationData, testData = userArtistData.randomSplit([40,40,20], 13)\n",
    "trainingData.cache()\n",
    "validationData.cache()\n",
    "testData.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1059637, 1000049, 1), (1059637, 1000056, 1), (1059637, 1000114, 2)]\n",
      "[(1059637, 1000010, 238), (1059637, 1000062, 11), (1059637, 1000123, 2)]\n",
      "[(1059637, 1000094, 1), (1059637, 1000112, 423), (1059637, 1000113, 5)]\n",
      "19769\n",
      "19690\n",
      "10022\n"
     ]
    }
   ],
   "source": [
    "print (trainingData.take(3))\n",
    "print (validationData.take(3))\n",
    "print (testData.take(3))\n",
    "print (trainingData.count())\n",
    "print (validationData.count())\n",
    "print (testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Recommender Model\n",
    "For this project, i will train the model with implicit feedback. You can read more information about this from the collaborative filtering page: http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "def cal_score(predict, actual):\n",
    "    if len(actual) < len(predict):\n",
    "#         print \"here\"\n",
    "        predict = predict[0:len(actual)]\n",
    "    return len(list(set(predict) & set(actual)))*1.0/len(actual)\n",
    "\n",
    "def modelEval(model, dataset):\n",
    "    # Find the list of all artists in the whole data set\n",
    "    all_artists = userArtistData.map(lambda x: x[1]).distinct().collect()\n",
    "    # Find the users in the input dataset\n",
    "    test_user = dataset.map(lambda p: p[0]).distinct().collect()\n",
    "    # Find the artists each user listened to in the training set and generate the test data\n",
    "    global trainingData\n",
    "    testdata = trainingData.filter(lambda x: x[0] in test_user).map(lambda x: (x[0], x[1])).groupByKey()\n",
    "    testdata = testdata.map(lambda x: (x[0], list(x[1])))\n",
    "    testdata = testdata.flatMap(lambda x: [(x[0],a) for a in all_artists if a not in x[1]])\n",
    "    # Find the artists each user listened to in the input dataset\n",
    "    testdata_actual = dataset.map(lambda x: (x[0], x[1])).groupByKey().map(lambda x: (x[0], list(x[1]))).collectAsMap()\n",
    "    predictions = model.predictAll(testdata).map(lambda x: (x[0], (x[1], x[2])))\n",
    "    predictions = predictions.groupByKey().map(lambda x: (x[0], sorted(list(x[1]), key=lambda y: y[1], reverse=True)))\n",
    "    predictions = predictions.map(lambda x: (x[0], cal_score([y[0] for y in x[1]], testdata_actual[x[0]])))\n",
    "    return predictions.map(lambda x:x[1]).reduce(lambda x, y: x+ y) * 1.0 / len(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model score for rank 2 is 0.09365101274010958\n",
      "The model score for rank 10 is 0.0948198831576256\n",
      "The model score for rank 20 is 0.07628028287919196\n"
     ]
    }
   ],
   "source": [
    "training = trainingData.map(lambda x: Rating(int(x[0]), int(x[1]), float(x[2])))\n",
    "for r in [2, 10, 20]:\n",
    "    model = ALS.trainImplicit(training, rank = r, seed=345)\n",
    "    s = \"The model score for rank %s is %s\" % (r, modelEval(model, validationData),)\n",
    "    print (s) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.059515526043292855\n"
     ]
    }
   ],
   "source": [
    "bestModel = ALS.trainImplicit(training, rank=10, seed=345)\n",
    "print (modelEval(bestModel, testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : Trying Some Artist Recommendations\n",
    "Using the best model above, predict the top 5 artists for user 1059637 using the recommendProducts function. Map the results (integer IDs) into the real artist name using artistAlias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist 0: Something Corporate\n",
      "Artist 1: My Chemical Romance\n",
      "Artist 2: Alkaline Trio\n",
      "Artist 3: The Used\n",
      "Artist 4: Further Seems Forever\n"
     ]
    }
   ],
   "source": [
    "recommended = map(lambda x: x.product, bestModel.recommendProducts(1059637, 5))\n",
    "for i, artist in enumerate(recommended):\n",
    "    a = \"Artist %s: %s\" % (i, artistData.lookup(artist)[0],)\n",
    "    print (a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
